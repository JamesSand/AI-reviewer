Summary: This paper identifies and analyzes the "momentum persistence effect" — the phenomenon that optimizer momentum/buffer persistence across discrete hard projections (e.g., SVD orthogonalization, spectral normalization) significantly amplifies the radial/momentum corruption that projections discard, explaining why soft (penalty-based) constraints often outperform hard projections in deep learning practice. The authors (1) demonstrate that a common implicit assumption in classical projected-gradient analyses — that momentum is reset at projections — is violated in practice; (2) provide controlled experiments on a constrained quadratic-on-sphere problem showing large quantitative and qualitative discrepancies between the classical ("reset") model and practical ("persistent") implementations; (3) derive a corrected, tractable recurrence-based model that predicts α^2 scaling, super-linear dependence on projection frequency τ, and saturation at a steady-state corruption; and (4) validate the mechanism in large neural nets (Transformer OSPA and ResNet with spectral normalization), showing consistent performance gaps favoring soft constraints and directly measuring large accumulated corruption in real models. The paper proposes practical design guidance (prefer soft constraints; if using hard projections, co-design with optimizer, reduce τ or reset momentum).

Soundness: 3
Presentation: 3
Contribution: 4

Strengths:
- Important, concrete observation that bridges a pervasive empirical phenomenon and a concrete implementation detail (momentum persistence). The contribution connects theory assumptions to practical optimizer implementations in a way that is directly actionable.
- Clean, controlled sphere/quadratic experiments that isolate the mechanism and show large quantitative discrepancies with classical theory. The recurrence model is simple and explains main empirical scalings (α^2, amplified τ-dependence, saturation).
- Empirical validation in two realistic domains (Transformer orthogonality; ResNet spectral normalization) strengthens the claim of practical relevance and generality beyond toy problems.
- Actionable design guidance for practitioners and a clear agenda for future theory (account for optimizer state when analyzing constrained optimization).
- Reasonable amount of experimental detail and appendices; demonstrations of saturation and amplification factors.

Weaknesses:
- The theoretical analysis relies on a heuristic (Assumption 4: approximate decorrelation between momentum and position so E[(m^T w)^2] ≈ E[||m||^2]/d). This is explicitly acknowledged, and the model is therefore not a fully rigorous first-principles proof. The heuristic is plausible in high-dimensions but the lack of rigorous justification weakens theoretical soundness for some readers.
- Treatment of adaptive optimizers is insufficiently detailed. The simplified analysis and main experiments focus on SGD-with-momentum (β) dynamics; yet many of the claims and some experiments involve Adam. Adam maintains both first- and second-moment estimates (m and v). The paper does not analyze how v (the adaptive scaling) interacts with projections (e.g., does persistence of v amplify or mitigate corruption?), nor does it show whether projecting or resetting v changes outcomes. This is an important practical omission.
- Some experimental choices and reporting could be stronger: the ResNet experiments use only 3 seeds (small), R² of theory fit is modest (0.54) and deserves discussion, and reported absolute corruption units (e.g., 900) lack intuitive normalization/interpretation across architectures. It is also unclear whether soft/hard variants were matched fairly in terms of hyperparameter tuning / compute budget (soft constraints sometimes require tuning λ).
- Limited exploration of mitigation strategies: the paper mentions “explicit momentum resets after projections” and co-design but does not present extensive empirics testing practical mitigations (e.g., projecting the momentum vector onto the tangent space, resetting only first moment but preserving second moment, partial damping, calibrating λ, or using optimizer-aware projection updates). Showing which mitigations work in practice (and at what cost) would strengthen the practical contribution.
- Scope limits: the theory is developed for sphere/quadratic problems and heuristically extended; generalization to arbitrary manifolds or complex constraint geometries (e.g., Stiefel manifold for matrices vs. simple sphere) is argued but not derived. Theoretical results for adaptive optimizers and more general manifolds are left as future work.

Questions:
1. Assumption 4 justification and sensitivity: can the authors (a) provide empirical evidence that the decorrelation heuristic holds across dimensions, β, and τ (e.g., measure correlation between m and w over time in experiments), and (b) quantify how sensitive the model predictions (scaling exponents, amplification factor) are to this approximation?
2. Adam and adaptive moments: how does the second-moment buffer v in Adam affect corruption dynamics? Do projections interact with v in a way that amplifies or reduces the persistence effect? Have you measured corruption evolution separately for Adam’s m and v, and can you report experiments that reset or project m and/or v at projection time?
3. Practical mitigations: did you evaluate simple mitigation strategies in real neural models, such as (a) projecting the momentum vector onto the tangent space alongside parameters, (b) zeroing or scaling the first moment m at each projection step, (c) damping (multiplying m by factor <1) after projection, or (d) infrequent projection + small learning rate? Which of these yield best trade-offs performance vs optimization speed?
4. Per-layer / per-parameter behavior: the ResNet and Transformer experiments aggregate corruption across the network. Is the corruption concentrated in some layers (e.g., heads, large matrices) or uniform? Layerwise measurements would help practitioners know where to focus mitigation.
5. Hyperparameter parity and tuning: for the neural experiments, were soft and hard variants given equal hyperparameter search budgets (learning rates, λ, τ)? Could the performance gap be partly due to suboptimal λ tuning for soft constraints or to SVD/penalty computational differences (affecting effective training budgets)?
6. Reproducibility: please provide code (or a pointer to a repo) and exact seeds/commands for the main NN experiments. Also report wall-clock overhead of SVD projections and penalty computations so readers can evaluate the practical cost of soft vs hard choices.
7. Interpretation of large corruption magnitudes: how should practitioners interpret values like "corruption grows to 900"? Is there a scale-invariant way to present corruption (e.g., normalized by parameter norm, per-parameter average, or relative to gradient magnitude) so readers can compare across models?
8. Connection to Riemannian methods: can projecting the momentum into the tangent space be interpreted as a lightweight Riemannian correction? Any preliminary experiments or theoretical commentary would be helpful.

Flag For Ethics Review: No. The work presents theory and optimization experiments on public benchmark tasks and standard models; there are no apparent ethical issues (no sensitive personal data, no deployment/use harms obvious). If the authors release code or pre-trained models, standard license and dataset usage considerations apply but do not require an ethics review.

Rating: 6

Confidence: 4

Code Of Conduct: Yes