# Review of "The Momentum Persistence Effect: A New Theory for Why Soft Constraints Outperform Hard Projections"

## Summary

This paper identifies the "momentum persistence effect" as an explanation for why soft, penalty-based constraints often outperform hard projections in deep learning. The authors argue that classical optimization theory implicitly assumes momentum resets after projections, contradicting all practical implementations (Adam, SGD). Through controlled experiments on a quadratic sphere optimization problem, they demonstrate that momentum persists across projections, creating compounding corruption that saturates at levels 5-7× higher than theoretical reset models predict. They validate these principles in Transformers with orthogonal constraints (OSPA) and CNNs with spectral normalization, providing design principles for practitioners.

## Soundness

**Score: 3 (Good)**

**Strengths:**
- The core insight is valid and well-motivated: practical optimizers do maintain momentum across projections, while classical theory assumes resets
- Controlled experiments systematically compare reset vs. persistent momentum variants, clearly isolating the effect
- The theoretical framework (Theorem 2) correctly models a linear recurrence and predicts saturation behavior
- Extended experiments (200 cycles) validate saturation predictions with reasonable agreement (R² = 0.54)
- Neural network validation demonstrates the effect manifests in realistic architectures

**Significant Weaknesses:**

1. **Theoretical Rigor:** The theoretical analysis relies critically on **Assumption 4** (approximate decorrelation), which the authors acknowledge as a "heuristic approximation" without "rigorous magnitude predictions" (p. 580). They explicitly state: "a fully rigorous analysis without this assumption is a major theoretical challenge" (p. 753). This fundamentally limits claims about theoretical contributions. The authors properly acknowledge this limitation, but it prevents the work from being a rigorous theoretical advance.

2. **Theory-Practice Gap:** 
   - Theory predicts amplification factor of (1 - β^{2τ})^{-1} ≈ 7.2 for experimental parameters
   - Experiments show only 5.5× amplification (30% discrepancy)
   - Model fit explains only R² = 0.54 of variance
   - This gap deserves deeper investigation; the heuristic approximation may be missing important factors

3. **Limited Scope of Theoretical Analysis:**
   - Only quadratic problems on unit spheres analyzed
   - Generalization to other constraint manifolds and non-quadratic objectives remains unclear
   - The analysis doesn't explain *why* soft constraints perform better—only that hard projections cause corruption
   - No convergence guarantees or optimization convergence rates provided

4. **Experimental Limitations:**
   - Neural network experiments show modest performance gaps (0.8% CNN, 1-6% Transformer)
   - Low-data OSPA results are the most compelling but based on limited data
   - Only 3-5 seeds for NN experiments; statistical significance testing is absent
   - No confidence intervals on performance gaps
   - CNN experiments use only SGD; Adam experiments limited to Transformers

5. **Incomplete Causal Analysis:**
   - The mechanism is correctly identified (stale momentum), but *why* this causes performance degradation isn't fully explained
   - Is performance loss due to: constraint violations? Noisy descent directions? Information loss?
   - Connection to soft constraints avoiding this effect could be more rigorous

## Presentation

**Score: 3 (Good)**

**Strengths:**
- Clear narrative arc: identify problem → show classical theory failures → propose mechanism → validate
- Excellent use of Figure 1 comparing reset vs. persistent momentum across four diagnostic plots
- Well-structured appendix with detailed protocols
- Transparent discussion of theoretical limitations

**Weaknesses:**
- **Central claim framing:** The paper claims to "explain" why soft constraints outperform hard projections, but actually explains why *hard projections with persistent momentum* cause corruption. This is slightly different and could be stated more precisely
- Main text could briefly summarize key theoretical assumptions earlier
- The connection between momentum corruption magnitude and performance degradation could be more explicit
- Some notation introduced without immediate definition (e.g., momentum corruption ∆m_t in Eq. 2)
- "Momentum reset" terminology might confuse with other meanings in optimization literature

## Contribution

**Score: 3 (Good)**

**Contributions Present:**
1. **Identifying the gap:** Demonstrates that classical theory's implicit "momentum reset" assumption is violated in practice—a genuine insight (novelty ✓, clarity ✓)
2. **Empirical characterization:** Provides extensive experimental evidence of super-linear scaling laws (τ^{1.5-2.0}, α^{1.5-2.0}) with detailed quantification
3. **Theoretical modeling:** Develops a recurrence relation model predicting saturation behavior, though with limitations (noted above)
4. **Design principles:** Offers actionable guidance: prefer soft constraints, co-design hard projections with optimizers, avoid frequent projections with high learning rates
5. **Cross-domain validation:** Demonstrates phenomenon in both NLP (Transformers) and vision (CNNs)

**Significance Assessment:**
- **Impact on practice:** Moderate. The recommendation to use soft constraints over hard projections is already widely adopted; the paper provides first theoretical justification but doesn't change practice substantially
- **Impact on theory:** Moderate-to-Limited. Identifies an important theory-practice gap but doesn't provide fully rigorous theoretical resolution. Heuristic approximations and R²=0.54 fits leave room for deeper understanding
- **Novelty:** Good. While momentum issues in optimization are recognized (SPAM optimizer, AdamW analysis), systematic analysis of *projection-induced* momentum corruption with this level of detail appears novel

**Missing Elements:**
- No convergence analysis (rates, final solution quality)
- Limited treatment of how this interacts with other optimization phenomena (gradient clipping, normalization schemes, etc.)
- No algorithmic innovations proposed

## Strengths

1. **Systematic experimental validation:** The controlled sphere experiments with explicit reset/persist variants clearly isolate the effect. The ability to directly compare is methodologically strong
2. **Practical relevance:** Addresses a real phenomenon observable across GANs, Transformers, and vision models
3. **Honest presentation:** Authors transparently discuss limitations of Assumption 4 and model fit quality (R²=0.54)
4. **Reproducibility:** Detailed protocols in appendices; hyperparameters specified; multiple seeds reported
5. **Integration of theory and practice:** Section connecting simplified model to complex architectures is well-executed
6. **Clear performance gap in challenging regime:** 6.1% improvement on SST-2 with 10% data is compelling evidence

## Weaknesses

1. **Theoretical incompleteness:**
   - Assumption 4 (decorrelation) lacks rigorous justification
   - 30% gap between predicted (7.2×) and observed (5.5×) amplification unexplained
   - No convergence theory
   - Analysis limited to quadratic, sphere-constrained problems

2. **Limited scope of neural network validation:**
   - Only two case studies (orthogonal constraints, spectral norm)
   - Modest performance gaps in most settings (< 2%)
   - Statistical significance not formally tested
   - No experiments with other constraint types (e.g., Lipschitz constraints, weight clipping)

3. **Incomplete mechanistic explanation:**
   - Why does momentum corruption specifically hurt performance?
   - How does this interact with batch normalization, layer normalization?
   - What's the relationship to constraint violation magnitude?

4. **Missing practical guidance:**
   - When should practitioners use hard projections despite momentum persistence?
   - How to choose projection frequency optimally?
   - How to design optimizer-aware constraint enforcement?

5. **Presentation gaps:**
   - Main text doesn't clearly state the heuristic nature of the theoretical analysis early enough
   - Could more explicitly frame contribution as "explaining failure mode of hard projections" vs. "explaining success of soft constraints"

## Questions for Authors

1. Can you provide intuition for why the theoretical amplification factor (7.2×) exceeds empirical (5.5×) by 30%? What does the 0.46 of unexplained variance in the R²=0.54 fit represent?

2. How does momentum persistence interact with adaptive learning rate schemes (Adam's adaptive schedules)? Does the corruption effect compound differently?

3. For the CNN experiments, why is the performance gap (0.8%) substantially smaller than Transformers? Is this problem-dependent or related to SGD vs. Adam?

4. Can Assumption 4 (decorrelation) be validated empirically? Plot the empirical correlation between momentum direction and parameter position?

5. Have you tested other manifolds (Stiefel, hyperbolic, etc.)? Does the persistence effect generalize?

6. What happens with other constraint enforcement methods (retraction-based, exponential maps)? Are these subject to the same effect?

## Minor Issues

- Line 243: "5.5×" should be consistently formatted throughout
- Figure 1(d): Error bars would strengthen the amplification factor comparison
- Appendix A.9: The distinction between "analytical model" and "rigorous proof" could be clearer in main text
- Table 3: Confidence intervals missing on performance gaps

## Ethics

**Flag For Ethics Review: No**

The paper is a theoretical/empirical study with no apparent ethical concerns. No human subjects, sensitive data, or potential for misuse of the findings. The research is straightforward optimization theory.

## Overall Assessment

This paper makes a solid empirical contribution by identifying and characterizing the momentum persistence effect—a genuine gap between classical optimization theory and modern practice. The experimental work is rigorous and the core insight is valuable. However, the theoretical contribution is limited by reliance on heuristic approximations and the practical impact is moderate (confirming existing wisdom about soft constraints). The work succeeds as an explanation of an empirical phenomenon but falls short of providing a complete theoretical foundation or significant algorithmic innovations.

The paper is above the bar for a top-tier venue but not substantially. It would benefit from:
- More rigorous theoretical analysis (or honest downgrade of theory claims)
- Broader neural network validation across constraint types
- Deeper mechanistic explanation of why momentum corruption reduces performance
- Novel algorithmic contributions leveraging these insights

---

## Recommendation

**Rating: 6 (Marginally Above Acceptance Threshold)**

**Confidence: 4 (High)**

The paper makes a genuine contribution to understanding the theory-practice gap in constrained optimization and provides valuable design principles for practitioners. However, theoretical limitations and modest practical improvements prevent it from being a strong accept. The honest treatment of assumptions and transparent reporting of results is commendable. For acceptance at a venue like ICLR, the empirical validation across domains and clear practical guidance likely justify acceptance, though the work would benefit significantly from addressing the theoretical gaps.