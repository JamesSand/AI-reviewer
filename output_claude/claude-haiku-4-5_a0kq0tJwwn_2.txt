# Review of "The Momentum Persistence Effect: A New Theory for Why Soft Constraints Outperform Hard Projections"

## Summary

This paper identifies and analyzes the "momentum persistence effect," a previously unaccounted-for mechanism in constrained optimization theory that explains why soft penalty-based constraints often outperform hard projections in deep learning. The core insight is that classical constrained optimization theory implicitly assumes momentum buffers reset after each projection—an assumption violated by all standard optimizers (Adam, SGD). Through controlled experiments on quadratic optimization on a unit sphere, the authors show that momentum persistence creates compounding corruption that saturates 5-7× higher than classical theory predicts, explaining observed super-linear scaling laws. Neural network validation via Orthogonal Subspace Projection Attention (OSPA) in Transformers and spectral normalization in CNNs supports the theory's applicability to realistic architectures.

## Soundness: 3

**Strengths:**
- The systematic refutation of classical theory is compelling: empirical results (Table 1) show dramatic failures across all classical predictions (scaling laws off by orders of magnitude, magnitude errors of ~10,000×)
- The controlled experiment (Section 3.1) cleanly isolates the momentum reset assumption by comparing reset vs. persistent variants with identical parameters
- Saturation prediction (Mk = M∞(1 - β^(2τk))) is validated experimentally with reasonable fit (R² = 0.54)
- Extended experiments over 200 projection cycles provide good evidence for steady-state behavior
- Neural network validation demonstrates the effect manifests in realistic models

**Weaknesses:**
- **Critical theoretical limitation (A.4):** Assumption 4 (approximate decorrelation E[(m^T w)²] ≈ (1/d)E[‖m‖²]) is acknowledged as heuristic but never rigorously justified. The authors admit this "does not provide rigorous magnitude predictions" and "does not provide a rigorous, first-principles proof." For a theory paper, this is a significant gap—the model captures qualitative scaling but magnitude predictions depend on an unjustified approximation
- **Incomplete convergence theory:** The paper derives saturation predictions but provides no convergence guarantees, optimization bounds, or characterization of convergence rates. Classical results (Nesterov, etc.) offer such guarantees; this analysis lacks them
- **Limited scope of theoretical analysis:** 
  - Analysis restricted to quadratic objectives on unit sphere; applicability to non-convex neural network losses is empirically validated but theoretically ungrounded
  - Assumptions 2-4 (small step size, independence, decorrelation) are restrictive and may not hold broadly
  - No analysis of how results extend to different constraint manifolds beyond the sphere
- **Magnitude discrepancy:** Theoretical amplification factor (1 - β^2τ)^(-1) ≈ 7.2 vs. experimental 5.5× suggests the model is not capturing the full mechanism (unexplained ~30% error)
- **No formal convergence analysis:** The paper doesn't prove that the saturation point is reached in finite time or characterize the rate of convergence—only empirically shows it occurs "around cycle 50"

## Presentation: 3

**Strengths:**
- Clear narrative arc: establishes classical failure → proposes mechanism → validates predictions
- Excellent use of Figure 1 to isolate persistence effect with direct side-by-side comparisons
- Well-structured appendix with complete derivations and implementation details
- Good use of tables summarizing theory vs. experiment

**Weaknesses:**
- **Dense mathematical exposition in Sections 2-3:** The transition from Theorem 1 to corrected model (Section 2.4) jumps to equation (3) without deriving it step-by-step. Full derivation is in appendix, making main paper harder to follow
- **Notation inconsistencies:** 
  - Uses both ∆m_t (corruption) and M_k (expected squared corruption) without always being clear on the distinction
  - C is defined differently in different contexts (e.g., C = (1-β)²/d vs. C_within)
- **Missing intuition:** The paper could better explain *why* (1 - β^2τ)^(-1) creates exponential amplification. The connection between inherited momentum β^τ m_(k-1)τ and the exponential form deserves more intuitive explanation
- **Presentation of limitations:** Section A.9 relegates important caveats about Assumption 4 to the appendix; these deserve more prominence in main paper
- **Figure quality:** Figure 2 panel (d) compares theoretical (1.1×) vs. experimental (5.4×) amplification factors but doesn't discuss the discrepancy clearly

## Contribution: 3

**Strengths:**
- **Novel mechanistic insight:** Identifying momentum persistence as the source of the soft-vs-hard constraint gap is genuinely new and addresses a long-standing empirical puzzle
- **Theory-practice bridge:** Explains why classical theory makes systematically incorrect predictions—a valuable contribution to understanding the theory-practice gap
- **Concrete design principles:** Provides actionable guidance (prefer soft constraints, co-design with optimizer choice, reduce projection frequency)
- **Empirical validation across domains:** OSPA (NLP), spectral normalization (vision) shows generality

**Weaknesses:**
- **Limited algorithmic novelty:** The paper diagnoses a problem but proposes no new algorithms or methods to mitigate momentum corruption. Design principles are largely "use soft constraints" (already standard practice)
- **Narrow theoretical scope:** Theory only rigorously applies to quadratic problems on spheres; extension to neural networks is empirical only
- **Incremental practical impact:** While intellectually interesting, practitioners already prefer soft constraints (spectral norm, weight norm); the paper explains *why* but doesn't enable new capabilities
- **Incomplete theoretical framework:** No proposed solutions that maintain hard constraint guarantees while avoiding persistence effects
- **Limited scope of neural network experiments:**
  - Only 2 case studies (OSPA, spectral norm); both already-known techniques
  - OSPA improvements are modest (+1.5% → +6.1% on low-data)
  - CNN results show smaller gaps (+0.8%), raising questions about universality

## Strengths

1. **Systematic refutation of classical theory through controlled experiments:** Table 1 provides damning evidence that classical predictions fail qualitatively and quantitatively
2. **Clean isolation of the key assumption:** Section 3.1's direct comparison of reset vs. persistent momentum cleanly demonstrates the source of failure
3. **Comprehensive empirical validation:** Extended experiments (200 cycles), saturation detection (Figure 2c), direct corruption measurement in CNNs (Figure 3)
4. **Well-motivated problem:** The soft vs. hard constraint puzzle is real and widespread in practice
5. **Honest treatment of limitations:** Appendix A.9 clearly states the heuristic nature of Assumption 4 and what remains open

## Weaknesses

1. **Heuristic approximation undermines theoretical claims:** Assumption 4 is not rigorously justified. The authors acknowledge the need for "tools from stochastic differential geometry" that they don't provide. This weakens claims about being a rigorous "theory"
2. **Magnitude predictions unreliable:** ~30% error in amplification factor suggests the model is incomplete. Practical guidance based on these magnitudes could be misleading
3. **No convergence theory:** Missing formal analysis of convergence rate, finite-time guarantees, or bounds on optimization error. Classical optimization theory provides such results; this paper does not
4. **Limited algorithmic contribution:** No proposed solutions. "Use soft constraints" is already standard; paper doesn't enable new methods
5. **Modest practical improvements in neural networks:** 
   - +6.1% on low-data SST-2 but +0.8% on standard CNN task
   - Gap suggests effect strength depends heavily on regime, not fully characterized
6. **Restricted theoretical scope:** Analysis limited to quadratic + sphere. Applicability to arbitrary non-convex constraints is empirically shown but theoretically ungrounded
7. **Some experimental limitations:**
   - Neural network validation uses only 3-5 seeds (small sample)
   - No comparison to Riemannian optimization methods mentioned in intro
   - Low-data SST-2 is small sample (10% of limited set); statistical reliability questionable
8. **Theory-experiment gap:** Predicted vs. empirical amplification factors (7.2× vs. 5.5×) suggests underlying mechanism not fully captured

## Questions

1. Can you provide a rigorous justification for Assumption 4 (decorrelation), or sketch what tools from stochastic differential geometry would be needed?
2. Why does the amplification factor discrepancy (7.2× theory vs. 5.5× empirical) persist across experiments? What mechanism is the model missing?
3. How does momentum persistence affect convergence rate? Can you provide finite-time convergence guarantees?
4. How does the effect scale with dimension d? Does the (1/d) factor in Assumption 4 mean the effect diminishes in high dimensions?
5. Have you compared to Riemannian SGD with adaptive methods (Bécigneul & Ganea 2019b), which you cite? How large is the practical gap?
6. For the CNN experiment, why is the performance gap (+0.8%) so much smaller than Transformers (+6.1%)? Does this suggest the effect is task/architecture dependent?
7. Can explicit momentum resets after projections (suggested design principle) be justified theoretically? At what cost to optimization?

## Flag For Ethics Review: No

The paper studies optimization algorithms with no obvious ethical concerns. Recommendations favor soft constraints (standard practice) and do not involve sensitive applications, protected attributes, or potential harms.

## Rating: 5

**Justification:** The paper makes a valuable mechanistic contribution by identifying momentum persistence as the source of the soft-vs-hard constraint gap and provides extensive empirical validation. However, the theoretical framework relies on an unjustified heuristic approximation (Assumption 4), lacks convergence analysis, and offers limited algorithmic novelty beyond "use soft constraints." The work is intellectually interesting and addresses a real phenomenon but falls short of the rigor expected for a top-tier venue's theoretical contributions. The empirical validation is solid but not sufficiently novel on the application side.

For acceptance at ICLR (30% threshold), the paper would need: (1) a more rigorous theoretical foundation or honest framing as "empirically-validated model," (2) proposed algorithms that maintain hard constraint guarantees while mitigating persistence effects, or (3) stronger neural network validation showing consistent practical improvements across diverse domains.

**Comparison to bar:** This is a borderline paper with genuine insight but unresolved theoretical gaps. It makes a solid empirical contribution to understanding practice but doesn't quite reach the standard for a top venue in either theoretical rigor or algorithmic innovation.

## Confidence: 4

Strong confidence in assessment of empirical results and experimental design. Lower confidence in theoretical evaluation—the field may have standards for heuristic models that I'm not fully calibrating to, and stochastic differential geometry tools may provide more justification than apparent. However, the magnitude discrepancy and lack of convergence theory remain clear limitations regardless of how rigorously Assumption 4 might be justified.

---

## Code of Conduct

Yes. This review was prepared according to standard double-blind peer review principles with attention to constructive feedback and technical accuracy.