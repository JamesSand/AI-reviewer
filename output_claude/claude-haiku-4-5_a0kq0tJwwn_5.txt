# Review of "The Momentum Persistence Effect: A New Theory for Why Soft Constraints Outperform Hard Projections"

## Summary

This paper identifies and analyzes the "momentum persistence effect," a previously unaccounted-for mechanism explaining why soft, penalty-based constraints often outperform hard projections in deep learning. The key insight is that classical constrained optimization theory implicitly assumes momentum buffers reset after each projection—an assumption contradicted by standard implementations like Adam and SGD with momentum. Through controlled experiments on quadratic optimization and validation in neural networks (Transformers and CNNs), the authors demonstrate that when momentum persists across projections, corruption accumulates to levels 5-7× higher than predicted by classical theory, saturating rather than growing indefinitely. The paper provides a corrected theoretical model predicting this saturation and super-linear scaling laws with learning rate and projection frequency.

---

## Soundness: 3 (Good)

**Strengths:**
- The empirical discovery is rigorous: direct comparison of reset vs. persistent momentum (Figure 1, Section 3.1) cleanly isolates the critical assumption and makes the theory-practice gap unmistakable
- Systematic violation of all classical predictions (Table 1) provides compelling evidence
- The corrected model successfully predicts saturation behavior with R² = 0.54 (Figure 2)
- Extended experiments to 200 projection cycles provide strong validation
- Two independent case studies (OSPA + spectral norm) demonstrate generality
- Direct measurement of momentum corruption in ResNet-18 (901 units) provides powerful empirical support

**Weaknesses:**
- **Critical heuristic approximation**: Assumption 4 (approximate decorrelation between momentum direction and position) lacks rigorous justification. The authors acknowledge this is necessary for tractability but provide only empirical validation. The theoretical framework is not fully rigorous in a mathematical sense.
  - Magnitude predictions differ from classical theory by order of magnitude (10,000×), yet the model achieves R² = 0.54, suggesting the scaling laws may be more robust than the underlying approximation
  - A complete first-principles derivation would require tools from stochastic differential geometry on manifolds, which remains open
- **Limited theoretical scope**: The analysis applies specifically to quadratic objectives on the unit sphere. Generalization to arbitrary constraint manifolds and nonconvex objectives is not formally established, though neural network experiments suggest the mechanism is robust
- **Missing convergence analysis**: No formal convergence guarantees are provided for the corrected model. The saturation prediction is empirically validated but theoretically incomplete
- **Noise model simplification**: Assumes i.i.d. Gaussian noise, which may not capture structured noise in deep learning
- **Incomplete justification of super-linear τ scaling**: While the factor (1 − β^{2τ})^{−1} explains the phenomenon, the precise exponent 1.5-2.0 in experiments vs. formula predictions shows room for refinement

---

## Presentation: 3 (Good)

**Strengths:**
- Clear narrative structure: builds from classical failure → discovery → corrected model → validation
- Well-designed figures (Figures 1-2) effectively communicate key comparisons with informative panels
- Explicit statement of assumptions in Definitions and formal theorems aids clarity
- Section 3.1 isolates the mechanism elegantly through the crucial experiment
- Appendix provides comprehensive derivations and experimental protocols
- Design principles in Section 5 are actionable and well-motivated

**Weaknesses:**
- **Assumption 4 disclosure**: While mentioned, the significance of the heuristic approximation could be emphasized more prominently in the main text. Readers may not fully appreciate that magnitude predictions rely on an unproven assumption
- **Notation density**: Heavy notation (β^{2τ}, C = (1−β)²/d) becomes cumbersome; more intuitive parameterizations (e.g., persistence factor) might help
- **OSPA methodology not fully clear**: The paper mentions Orthogonal Subspace Projection Attention but provides limited intuition for why this constraint applies to attention mechanisms
- **Figure 2 panel (d) hard to parse**: The theoretical vs. experimental amplification factor comparison could use clearer labeling
- **Missing discussion of when soft fails**: The paper doesn't discuss scenarios where soft constraints might underperform (e.g., when exact constraint satisfaction is essential)

---

## Contribution: 4 (Excellent)

**Contributions:**

1. **Identification of fundamental blind spot** (Major): The discovery that classical constrained optimization theory models an incorrect algorithm (with momentum reset) is significant. This resolves a long-standing empirical puzzle in deep learning.

2. **Quantitative empirical invalidation of classical theory** (Major): Showing systematic 10,000× underestimation of corruption magnitudes and qualitatively wrong scaling laws (Table 1) is a strong contribution that will motivate rethinking classical assumptions.

3. **Corrected theoretical model with saturation predictions** (Major): The recurrence relation formulation (Equation 24, Theorem 2) successfully captures practical optimizer dynamics and predicts saturation behavior—something classical theory cannot.

4. **Validation across domains** (Good): Testing on Transformers (OSPA) and CNNs (spectral norm) with direct measurement of momentum corruption (ResNet-18: 901 units) demonstrates the effect is not specific to toy problems.

5. **Actionable design principles** (Good): Section 5 provides practitioners with clear guidance (prefer soft constraints, co-design with optimizer choice).

**Significance:**
- Addresses a widely-observed but theoretically-unexplained phenomenon
- Reveals limitations of classical optimization theory that should influence future theoretical work
- Practical impact: justifies soft constraint use across vision and NLP architectures
- Opens new research directions (optimizer-constraint co-design, constraint-aware optimizers)

**Limitations of contribution:**
- The theory remains incomplete (heuristic approximations, no convergence analysis)
- Neural network validation shows smaller gaps than controlled experiments (0.8-6.1% vs. 5-8× amplification), raising questions about how dominant the effect is in practice
- Does not propose new algorithms; contribution is primarily diagnostic

---

## Strengths

1. **Definitive experimental design**: The controlled comparison of reset vs. persistent momentum (Figure 1) is a textbook example of isolating a variable. This is the paper's strongest contribution.

2. **Comprehensive empirical validation**: 50 trials per configuration, extended to 200 cycles, 5 random seeds for neural networks, and direct measurements of corruption provide high confidence in findings.

3. **Orders-of-magnitude discrepancies**: A 10,000× error in absolute magnitudes is impossible to dismiss and demonstrates classical theory fundamentally fails at predicting practical optimizer behavior.

4. **Clear identification of mechanism**: The term β^τ m_{(k-1)τ} is the precise mathematical locus of the problem, making the discovery concrete and testable.

5. **Practical relevance**: The findings directly explain why practitioners prefer soft constraints across multiple domains (GANs, Transformers, RNNs, CNNs).

6. **Honest about limitations**: Authors explicitly acknowledge Assumption 4 is heuristic and discuss open theoretical challenges.

---

## Weaknesses

1. **Incomplete theory**: 
   - Assumption 4 (decorrelation) is justified only empirically
   - No formal convergence analysis for the corrected model
   - Saturation model fits with R² = 0.54, leaving 46% unexplained variance
   - Limited to quadratic objectives on spheres

2. **Theory-practice gap in neural networks**:
   - Controlled experiments show 5.5-8.8× amplification
   - Neural network results show much smaller gaps (0.8-6.1% on SST-2)
   - Why is the effect so much weaker in practice? This suggests missing factors or that corruption is not the only relevant mechanism

3. **Limited algorithmic contribution**: The paper is primarily diagnostic. It explains why soft constraints work better but doesn't propose better hard projection methods or new optimizers that mitigate the effect beyond "reset momentum" (which is impractical).

4. **Constraint manifold generality unclear**: 
   - Sphere analysis is clean but representative?
   - How does the mechanism behave for other manifolds (Stiefel, SO(n), etc.)?
   - OSPA and spectral norm provide evidence but are not rigorous generalizations

5. **Hyperparameter sensitivity in neural networks**:
   - Best-performing configurations selected via grid search
   - Results may be sensitive to hyperparameter choices (λ for soft, τ for hard)
   - No ablation studies on regularization strength

6. **Noise model limitations**: Assumes i.i.d. Gaussian noise; structured gradient noise or batch effects in neural networks could behave differently.

7. **Missing comparisons**:
   - No comparison to Riemannian optimization methods (mentioned but not empirically evaluated)
   - No comparison to momentum resets in neural networks (mentioned as impractical but worth measuring)

---

## Questions

1. **On Assumption 4**: Have you explored alternative approximations (e.g., worst-case bounds instead of decorrelation) that could be rigorously justified? How sensitive are predictions to violations of this assumption?

2. **Theory-practice gap in networks**: Why is the amplification factor 5.5× in controlled settings but only 0.8-6.1% improvement in SST-2? Are there other mechanisms at play (e.g., constraint geometry more favorable in Transformers)?

3. **Riemannian methods**: Why not compare empirically to Riemannian-geometric optimizers that should avoid momentum persistence issues? This would strengthen the claim that soft constraints are the best practical approach.

4. **Generalization to other manifolds**: Can you sketch how the analysis would extend to Stiefel or orthogonal groups? Is the saturation mechanism universal?

5. **Momentum reset cost**: You mention explicit momentum resets are "impractical"—have you measured the actual wall-clock cost? Is this assumption necessary?

6. **Learning rate schedules**: Do decaying learning rates (common in practice) change the saturation level? Your experiments use fixed α.

7. **Condition number dependence**: Theorem 1 predicts inverse κ-dependence under classical theory, but experiments show positive correlation (κ^{0.3}). Does your corrected model explain this better?

---

## Flag For Ethics Review: No

No ethics concerns identified. The paper is theoretical/experimental work on optimization—not involving human subjects, sensitive data, or dual-use applications.

---

## Minor Issues

- **Line 107**: "Assumption 1" definition could clarify it's implicit, not explicit, in classical work
- **Section 2.3**: Fitted exponents (1.7 ± 0.1, etc.) would benefit from confidence intervals, not just standard errors
- **Figure 1 caption**: "(Reality)" is colloquial; use "(Practical Implementation)" for consistency
- **Table 2**: Standard deviations reported; confidence intervals would be more informative
- **Appendix A.9**: "Validated analytical model" is slightly overstated; "empirically-validated analytical model" more accurate given Assumption 4
- **Missing**: How does batch size affect the noise level σ² and thus corruption predictions?

---

## Recommendations for Improvement

1. **Strengthen theoretical foundation**: Provide rigorous bounds (even if looser) to replace Assumption 4, or frame theory as "empirically-validated mechanism" more explicitly
2. **Close theory-practice gap**: Carefully analyze why neural network improvements (~1-6%) are much smaller than controlled amplification (~5×); this discrepancy needs explanation
3. **Algorithm design**: Propose practical methods to mitigate momentum persistence (e.g., adaptive momentum damping near constraint boundaries)
4. **Riemannian comparison**: Include empirical comparison to constraint-aware optimizers
5. **Extended analysis**: Discuss how architecture (attention head dimension, layer depth) affects corruption levels

---

## Overall Assessment

This is a solid paper that makes a genuine contribution to understanding a theory-practice gap in constrained optimization. The key discovery—that classical theory assumes momentum reset, contradicting practice—is insightful and well-validated empirically. The corrected model successfully explains super-linear scaling and saturation behavior observed in practice.

However, the theoretical contribution is incomplete. The core model relies on a heuristic approximation (Assumption 4) justified only by empirical agreement, not rigorous argument. There is no convergence analysis. The theory applies specifically to quadratic objectives on spheres, though neural network experiments suggest generality. Additionally, the practical impact in neural networks appears more modest than the amplification factors in controlled settings, leaving open questions about which mechanisms matter most in realistic scenarios.

The paper would benefit from either (a) rigorous theoretical foundations with looser bounds, or (b) more thorough algorithmic contributions addressing the identified problems. As-is, it is a valuable empirical and diagnostic contribution suitable for a top venue, but not a complete theoretical advance.

---

## Rating: 6 (Marginally Above Acceptance Threshold)

**Justification:**
- **Accepts**: Strong empirical discovery with clear practical relevance; resolution of longstanding puzzle; validation across multiple domains; honest about limitations
- **Concerns**: Incomplete theory (heuristic assumptions); modest algorithmic contribution; theory-practice gaps in neural networks unexplained; narrower scope than suggested by generality claims

The paper makes meaningful contributions and would interest the ICLR community, particularly practitioners and optimization researchers. However, the incomplete theoretical foundation and modest practical improvements in neural networks prevent it from being a strong accept. It is above the acceptance threshold because the core discovery is sound and significant, but not by a large margin.

---

## Confidence: 4 (High Confidence)

The empirical work is rigorous and reproducible. The theoretical limitations are clearly stated and acknowledged. Neural network experiments follow standard protocols. The only uncertainty is whether the heuristic theory generalizes beyond the tested settings, but the paper's empirical validation provides reasonable confidence in the reported mechanisms.

---

## Code of Conduct

Yes, I have reviewed this submission impartially according to the stated criteria and conferences standards.