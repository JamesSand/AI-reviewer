# Review of "The Momentum Persistence Effect: A New Theory for Why Soft Constraints Outperform Hard Projections"

## Summary

This paper identifies and analyzes the "momentum persistence effect," a previously unaccounted mechanism that explains why soft, penalty-based constraints typically outperform hard projections in constrained optimization. The authors demonstrate that classical optimization theory implicitly assumes optimizer momentum resets after each projection—an assumption violated by standard implementations (Adam, SGD). Through controlled experiments on quadratic optimization on unit spheres and validation in Transformer and CNN models, they show that momentum persistence creates compounding corruption orders of magnitude larger than classical theory predicts, with saturation behavior and super-linear scaling laws. The paper provides practical design principles for constraint-optimizer co-design.

## Soundness: 3

**Strengths:**
- The core empirical observation is compelling: systematic violations of classical predictions across multiple scaling relationships (τ, α, κ)
- The controlled experimental design (comparing reset vs. persistent momentum variants) effectively isolates the hypothesized mechanism
- Extended validation over 200 projection cycles demonstrates saturation behavior
- The recurrence relation framework for modeling persistence is mathematically sound
- Cross-domain validation (Transformers and CNNs) strengthens generalizability claims

**Weaknesses:**
- **Critical heuristic approximation:** Assumption 4 (decorrelation between momentum direction and parameter position) lacks rigorous justification. The authors acknowledge this is "heuristic" and admits that "a complete mathematical justification remains an open theoretical challenge." While empirical validation is provided, this undermines the theoretical rigor for a paper presenting itself as providing new theory
- **Limited scope of theoretical analysis:** Derives results only for quadratic objectives on unit spheres. The claim that this generalizes to deep networks is empirically validated but not theoretically justified
- **Energy injection scaling (Lemma 2):** The connection between momentum update work and α² scaling is intuitive but lacks formal proof—the "work-energy relationship" appeal is informal
- **Model fitting quality:** R² = 0.54 for the saturation curve fit (Figure 2b) indicates the model explains only ~54% of variance. For a "validated" theoretical model, this is modest
- **Missing formal convergence analysis:** No convergence guarantees or rates proven for the corrected model. The analysis is primarily about steady-state corruption magnitude, not optimization convergence
- **Quantitative prediction gaps:** Theoretical prediction of 7.2× amplification vs. experimental 5.5× is close but represents a 30% discrepancy. For magnitude predictions, this is concerning

## Presentation: 3

**Strengths:**
- Clear narrative structure progressively building from classical model → empirical failures → corrected model
- Excellent use of figures (Figure 1 particularly effective in isolating the effect)
- Well-organized appendices with complete derivations
- Practical design principles clearly stated in Section 5

**Weaknesses:**
- **Overclaimed rigor in abstract/introduction:** Phrases like "we develop and validate a new theoretical model" and "accurately predicts" obscure the approximate nature of Assumption 4
- **Notation inconsistencies:** Mix of continuous and discrete formulations; kτ indexing could be clearer
- **Figure 2 interpretation:** The caption claims "excellent agreement" (R² = 0.54), which is overstated
- **Assumption 4 buried:** This critical limitation is relegated to Appendix A.1 rather than prominently featured in main text
- **Limited discussion of failure modes:** When does the persistence model break down? Under what problem structures might it not apply?

## Contribution: 3

**Strengths:**
- Identifies a previously unrecognized gap between classical optimization theory and practice
- Provides mechanistic explanation for widespread empirical phenomenon (soft constraints outperforming hard projections)
- Actionable principles for practitioners (prefer soft constraints, co-design with optimizer choice)
- Validates findings across multiple domains (quadratic optimization, NLP, vision)
- Successfully explains super-linear scaling relationships and saturation behavior

**Weaknesses:**
- **Somewhat incremental insight:** The core observation—that standard optimizers maintain momentum across projections—is implementation knowledge, not a novel algorithmic contribution. The theoretical contribution is explaining *why* this matters
- **Limited novelty in solutions:** The paper explains the phenomenon but doesn't propose new algorithms or methods beyond recommending existing soft constraints
- **Narrow experimental scope for neural networks:** Only two case studies (OSPA in BERT, spectral norm in ResNet-18). Performance gaps are modest (0.8-6.1%), making it unclear how practically significant the effect is across architectures
- **Generalization unclear:** The quadratic sphere model is simplified. How well do insights transfer to non-convex, high-dimensional neural network loss landscapes?
- **No comparison with Riemannian methods:** The paper mentions but doesn't compare with constraint-aware optimizers, limiting assessment of practical impact

## Strengths

1. **Systematic empirical investigation** with controlled variants that convincingly isolate momentum persistence as the causal mechanism
2. **Quantitative predictions** that mostly align with observations (orders of magnitude improvement over classical theory for corruption magnitude)
3. **Cross-validation** in realistic architectures demonstrates the effect has real impact, not just theoretical interest
4. **Well-written** with clear motivation connecting theory-practice gap to a concrete mechanism
5. **Honest about limitations** (Appendix A.9.1 explicitly discusses scope restrictions)
6. **Reproducible:** Detailed experimental protocols, hyperparameters, and multiple seeds reported

## Weaknesses

1. **Theoretical rigor compromised** by Assumption 4's heuristic nature and informal approximations (Lemmas 2)—for a theory paper, this is a significant issue
2. **Model fit quality insufficient** (R² = 0.54) to claim accurate prediction in steady state
3. **Limited algorithmic contribution:** Paper diagnoses a problem but proposes only existing solutions (use soft constraints)
4. **Moderate practical performance gains:** 0.8-6.1% improvements in neural networks are meaningful but not transformative, questioning practical importance
5. **Narrow scope of theory:** Results are for specific problem class (quadratic, sphere); generalization to arbitrary manifolds unproven
6. **Missing theoretical depth:** No convergence analysis, no formal characterization of when persistence helps vs. hurts
7. **Incomplete related work:** Limited discussion of recent work on optimizer-constraint interactions or manifest geometry in neural networks

## Questions

1. **Assumption 4 justification:** Can you provide any theoretical argument (e.g., concentration inequalities in high dimensions) for decorrelation between momentum direction and position? Or is this fundamentally heuristic?

2. **R² = 0.54:** What explains the 46% unexplained variance in Figure 2b? Are there systematic deviations between theory and experiment that you haven't captured?

3. **Quadratic problems only:** Have you attempted extending the theoretical analysis to non-convex objectives? Are there fundamental barriers?

4. **Algorithm design:** Why not propose an optimizer variant that explicitly manages momentum at constraint boundaries? This seems like a natural next step.

5. **Riemannian comparison:** How does OSPA-Hard compare to constraint-aware optimizers like those in Bécigneul & Ganea (2019)? This would contextualize the practical importance.

6. **High-noise regime:** You find the largest performance gap (6.1%) in low-data SST-2. Can you characterize the noise level at which soft constraints become substantially better?

7. **Generalization:** Do the scaling laws (α², super-linear τ) hold for constraints other than orthogonality/spectral norm?

## Flag For Ethics Review

**No.** The work is purely theoretical/empirical optimization research. No data collection from human subjects, no deployment of autonomous systems, no fairness/privacy concerns identified. Standard academic research ethics (reproducibility, honest reporting) are followed.

## Rating: 6

**Justification:**

This paper is **marginally above the acceptance threshold**. It makes a solid empirical contribution by identifying and characterizing the momentum persistence effect, with clear practical implications. The controlled experiments are well-designed and the cross-domain validation is convincing. The writing is generally clear and the problem is well-motivated.

However, the theoretical contribution is weakened by:
- Reliance on a heuristic approximation (Assumption 4) without rigorous justification
- Modest model fit quality (R² = 0.54)
- Limited algorithmic novelty (recommends existing soft constraints)
- Narrow scope of main theory (quadratic, sphere only)

The paper makes a *diagnosis* of an important theory-practice gap and explains it mechanistically, which is valuable. But it doesn't fully *resolve* the gap theoretically (the approximations are not rigorous) and it doesn't propose solutions (it explains why existing practice works, but doesn't innovate).

For a top-tier venue (30% acceptance rate), this represents competent work that advances understanding but has clear limitations preventing it from being a strong accept. It would benefit from:
1. Either proving or formally acknowledging the limits of Assumption 4
2. Extending theory beyond quadratic sphere
3. Proposing new algorithms leveraging these insights

**Borderline decision:** This could go either way depending on reviewer weights. If theoretical rigor is paramount, rate 5. If empirical insight and practical relevance are valued highly, rate 8.

## Confidence: 4

The empirical findings are clear and reproducible. The mechanistic explanation is compelling. However, the theoretical scope and rigor limitations create genuine uncertainty about the work's significance and generalizability to the broader class of constrained optimization problems in deep learning. Strong empirical validation partially offsets theoretical concerns.

---

## Code of Conduct
Yes, I have followed standard review practices: evaluated the work objectively on its merits, provided constructive feedback, identified both strengths and limitations, and made a reasoned recommendation based on stated criteria.